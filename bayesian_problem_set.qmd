---
title: "Bayesian methods problem set"
format:
  html:
    embed-resources: true
editor: visual
---

## Example code

```{r}
#| warning: FALSE

library(tidyverse)
library(patchwork)

#Build fake dataset
set.seed(100)
num_data_points <- 200
sd_data <- 0.25
par_true <- c(3, 0.5)
x <- runif(num_data_points, 0, 10)
y_true <- par_true[1] * (x / (x + par_true[2]))
y <- rnorm(length(y_true), mean = y_true, sd = sd_data)
```

```{r}
plot(x, y, ylim = c(0, par_true[1] + 2))
```

```{r}
#Set MCMC Configuration
num_iter <- 2000
num_pars <- 2
jump <- c(0.05, 0.05)

#Initialize chain
pars <- array(NA, dim = c(num_pars, num_iter))
pars[1, 1] <- 2
pars[2, 1] <- 1
log_likelihood_prior_current <- -10000000000
accept <- rep(0,num_iter * num_pars)

for(i in 2:num_iter){
  
  #Loop through parameter value
  
  for(j in 1:num_pars){
      #Randomly select new parameter values
    proposed_pars <- pars[, i - 1]
    proposed_pars[j] <- rnorm(1, mean = pars[j, i - 1], sd = jump[j])
    
    ##########################
    # PRIORS
    #########################
    #(remember that you multiply probabilities which mean you can add log(probability))
    log_prior <- dunif(proposed_pars[1], min = 0, max = 10, log = TRUE) + 
      dunif(proposed_pars[2], min = 0, max = 100, log = TRUE)
    
    #Likelihood.  
    #You could use:
    # pred <- process_model(x, pars = proposed_pars)
    # log_likelihood <- sum(dnorm(new_data, mean = pred, sd = sd_data, log = TRUE)
    # but we are looping here because it transitions well to the next section of the course
    log_likelihood <- rep(NA, length(x))
    pred <- rep(NA, length(x))
    for(m in 1:length(x)){
      ##########################
      # PROCESS MODEL
      #########################
      pred[m] <- proposed_pars[1] * (x[m] / (x[m] + proposed_pars[2]))
      ##########################
      # DATA MODEL
      #########################
      log_likelihood[m] <- dnorm(y[m], mean = pred[m], sd = sd_data, log = TRUE)
    }
    #Remember that you multiply probabilities which mean you can add log(probability)
    #Hence the use of sum
    log_likelihood <- sum(log_likelihood)
    
    ############################
    ###  PRIOR x LIKELIHOOD
    ############################
    #Combine the prior and likelihood
    #remember that you multiply probabilities which means you can add log(probability)
    log_likelihood_prior_proposed <- log_prior + log_likelihood
    
    #We want the ratio of new / old but since it is in log space we first
    #take the difference of the logs: log(new/old) = log(new) - log(old) 
    # and then take out of log space exp(log(new) - log(old))
    z <- exp(log_likelihood_prior_proposed - log_likelihood_prior_current)
    
    #Now pick a random number between 0 and 1
    r <- runif(1, min = 0, max = 1)
    #If z > r then accept the new parameters
    #Note: this will always happen if the new parameters are more likely than
    #the old parameters z > 1 means than z is always > r no matter what value of
    #r is chosen.  However it will accept worse parameter sets (P_new is less
    #likely then P_old - i.e., z < 1) in proportion to how much worse it is
    #For example: if z = 0.9 and then any random number drawn by runif that is
    #less than 0.90 will result in accepting the worse values (i.e., the slightly
    #worse values will be accepted a lot of the time).  In contrast, if z = 0.01
    #(i.e., the new parameters are much much worse), then they can still be accepted
    #but much more rarely because random r values of < 0.1 occur more rarely
    if(z > r){
      accept[i] <- 1
      pars[j, i] <- proposed_pars[j]
      log_likelihood_prior_current <- log_likelihood_prior_proposed
    }else{
      pars[j, i] <- pars[j, i - 1]
      log_likelihood_prior_current <- log_likelihood_prior_current #this calculation isn't necessary but is here to show you the logic
    }
  }
}

```

Creating a data frame with the parameters from each iteration

```{r}
#| warning: FALSE
d <- tibble(iter = 1:num_iter,
            par1 = pars[1, ],
            par2 = pars[2, ]) %>%
  pivot_longer(-iter, values_to = "value", names_to = "parameter")
```

Plot the MCMC chains and the posterior distributions 
```{r}
#| warning: FALSE

p1 <- ggplot(d, aes(x = iter, y = value)) +
  geom_line() +
  facet_wrap(~parameter, scales = "free") +
  theme_bw()

p2 <- ggplot(d, aes(x = value)) +
  geom_histogram() +
  facet_wrap(~parameter, scales = "free") +
  theme_bw()

p1 / p2
```

## Problem set

Your task is to modify the code above to estimate the posterior distribution of parameters in Q10 function that was in the likelihood analysis problem set (likelihood_problem_set.qmd). Use the same soil respiration data as used in the Q10 likelihood question.

The example uses the Michaelis-Menten equation and fake data.  You will NOT use this function and data in your answer.  Use the code as the template for your answer

**Question 1**: 

Provide the distribution name and distribution parameters describing your prior distributions. Justify why you chose the distribution and parameters. (do not spend time looking at the literature for values to use to build prior distribution - just give plausible priors and say why they are plausible)

**Answer 1:**

**Question 2:** 

Provide plots of your prior distributions. hist(rnorm(1000, mean = 2, sd = 1))

**Answer 2:**

```{r}


```

**Question 3:** 

Modify the code above to estimate the posterior distribution of your parameters. Put your modified code below.

**Answer 3:**

```{r}


```

**Question 4:** 

Plot the your MCMC chain for all parameters (iteration number will be the x-axis and parameter value will be the y-axis)

**Answer 4:**

```{r}


```

**Question 5:** 

Approximately how many iterations did it take your chain to converge to a straight line with constant variation around the line (i.e., a fuzzy caterpillar). This is the burn-in. If your chain did not converge, modify the `jump` variable for each parameters and/or increase your iterations. You should not need more than 10000 iterations for convergence so running the chain for a long period of time will not fix issues that could be fixed by modifying the `jump` variable. 

**Answer 5:**

**Question 6:** 

Remove the iterations between 1 and your burn-in number and plot the histograms for your parameters.

```{r}


```

**Answer 6:**

**Question 7:** Provide the mean and 95% credible intervals for each parameter. Use `quantile()` on each parameter.

```{r}


```

**Answer 7:**

**Question 8:** 

Randomly select 200 values from the parameters in your posterior distribution. Show the randomly selected values for each parameter as a histogram.

**Answer 8:**

```{r}


```

**Question 9:** 

Use the samples from Question 8 to generate posterior predictions of soil respiration at the observed temperature values (i.e., the same temperature data used in your model fit). Provide a plot with temperature on the x-axis and respiration on the y-axis. The plot should have the mean and 95% predictive uncertainty bounds (i.e., include uncertainty in parameters and in the data model).

If using tidyverse and ggplot, you can group_by(variable) and summarize to calculate the upper and lower creditable interval using `quantile()` function for each bound (e.g., `quantile(prediction, 0.025)` for the lower interval) and the mean using the `mean()`.  

See https://frec-5174.github.io/eco4cast-in-R-book/bayesian-methods-intro.html#predictive-posterior-distributions for example code.

**Answer 9:**

```{r}

```
